# ML-Based Strategy Research Pipeline

âš ï¸ **Project status: In Progress**  
This is an educational project built as part of my learning process while studying *Advances in Financial Machine Learning*.  
The full implementation is currently under development.

---

## ğŸ§  Project Purpose

This project explores how to transform noisy, unstructured financial transaction data into a structured, information-driven dataset suitable for machine learning.

The goal is not to "predict the market blindly", but to:

- Reduce noise
- Extract meaningful information
- Create informative labels
- Train ML models in a statistically disciplined way
- Avoid data leakage
- Integrate risk management directly into the modeling pipeline

This is a research-driven implementation inspired by modern ML practices.

---

# ğŸ” High-Level Architecture

The most stable conceptual architecture currently looks like:


Event Sampling
â†“
Feature Extraction
â†“
Symmetric Triple Barrier â†’ Directional Label
â†“
Model 1 (M1) â†’ Predict Side (Long / Short / Pass)
â†“
Asymmetric TBM / Path Filters â†’ Playability Label
â†“
Model 2 (M2) â†’ Act / Pass + Confidence
â†“
Dynamic Sizing & Execution
â†“
Validation (Purged CV + Embargo)


Each stage focuses on eliminating noise and extracting information with consistent statistical properties.

---

# ğŸ“¦ 1. Raw Data â†’ Structured Information

**Garbage in, garbage out.**

Financial transaction data is unstructured and noisy.  
To apply ML effectively, we:

1. Parse raw transaction data
2. Create information-driven bars
3. Regularize data into feature matrices

Instead of time-based bars, this project explores:

- Dollar Imbalance Bars
- Dollar / Tick Run Bars
- Information-driven sampling

The purpose is to recover better statistical properties:
- More stable variance
- More IID-like returns
- Equal information weight per observation

---

# ğŸ” 2. Feature Sampling & Double Filtering

After constructing statistically consistent bars:

### Step 1 â€” Feature Extraction

Each bar produces features describing market behavior:
- Imbalance measures
- Volatility measures
- Derived indicators
- Potential order-flowâ€“inspired features (planned)

ML models only see what we give them.  
Features must encode meaningful structure.

---

### Step 2 â€” CUSUM Filter (Persistence Detection)

Bars alone are not enough.

We apply a CUSUM filter to detect persistence:
- Only events where cumulative movement exceeds threshold `h`
- Threshold dynamically adjusted by volatility

This creates a **double filter**:

1. Information-driven bars
2. Persistence filter (CUSUM)

Result:
ML is trained only on statistically meaningful events â€” not random noise.

---

# ğŸ· 3. Labels & Model 1 â€” Direction Prediction

Once events are sampled and features extracted:

We need informative labels.

### Triple Barrier Method (Symmetric)

For each event:
- Upper barrier (TP)
- Lower barrier (SL)
- Time barrier (timeout)

Label encodes:
- Long
- Short
- No trade

Model 1 (M1) learns:
> Given these features at event time â†’ which direction historically had higher probability of hitting a barrier?

Key constraint:
- No forward data leakage
- Only past information available at decision time

---

# ğŸ§® 4. Model 2 â€” Risk Gate & Playability

Model 1 predicts *direction*.  
Model 2 evaluates *trade quality*.

Model 2 may use:
- Volatility features
- Regime context
- Path-dependent information
- Additional risk signals

It answers:
> Is this trade reasonable and executable under current conditions?

Outputs:
- Act / Pass
- Confidence score
- Risk-adjusted probability

---

# ğŸ“Š 5. Dynamic Position Sizing

Position sizing is determined by:

- Model 1 direction probability
- Model 2 confidence
- Volatility context
- Expected reward-to-risk profile

Decision thresholds are evaluated with:
- PnL-weighted error analysis
- FP/FN impact analysis
- Decision-theoryâ€“based calibration

The system aims to:
- Reduce trades with high SL probability
- Scale size based on probabilistic edge
- Avoid overconfident entries in unstable regimes

---

# ğŸ” 6. Trade Monitoring After Entry

After entry:

Each new bar:
- New features computed
- Updated probability estimates
- Optional tightening of SL
- Optional reduction or exit

Final outcomes logged for training:
- Barrier hit (TP / SL)
- Time expiry
- Return structure

Sequential modeling ensures:
- Proper information flow
- No leakage
- Realistic simulation

---

# ğŸ§ª 7. Validation Framework

Validation uses:

- Purged cross-validation
- Embargo techniques
- Proper temporal splits

Goal:
Prevent look-ahead bias and overfitting.

---

# ğŸ§± Conceptual Pipeline Summary


Raw Transactions
â†“
Information-Driven Bars
â†“
Feature Extraction
â†“
CUSUM Event Sampling
â†“
Triple Barrier Labeling
â†“
Model 1 (Direction)
â†“
Model 2 (Risk & Sizing)
â†“
Execution Logic
â†“
Outcome Logging
â†“
Retraining Loop


Every stage focuses on:

- Noise reduction
- Statistical consistency
- Information extraction
- Probabilistic decision-making

---

# ğŸ“Œ Current Status

ğŸš§ Implementation in progress.  
Core components under development:

- Modular backtesting engine
- Event sampling module
- Feature extraction pipeline
- Label generation system
- ML model integration
- Validation framework

This project is a structured learning exercise inspired by *Advances in Financial Machine Learning*, designed to deeply understand ML-based decision systems rather than merely replicate trading heuristics.

---

## ğŸ“ Learning Objective

The goal is not to build a production trading system.

The goal is to:

- Understand how to structure unstructured data
- Learn disciplined ML pipeline design
- Practice proper labeling and validation
- Integrate risk management into modeling
- Develop research-oriented thinking

---

More updates coming soon.